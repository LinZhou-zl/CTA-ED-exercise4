---
title: "CTA-ED Exercise 4: Scaling techniques"
author: "[Your full name here]"
date: "6/03/2024"
output: html_document
---

# Introduction

The hands-on exercise for this week focuses on: 1) scaling texts ; 2) implementing scaling techniques using `quanteda`. 

In this tutorial, you will learn how to:
  
* Scale texts using the "wordfish" algorithm
* Scale texts gathered from online sources
* Replicate analyses by @kaneko_estimating_2021

Before proceeding, we'll load the packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(dplyr)
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textmodels) # for estimating similarity and complexity measures
library(quanteda.textplots) #for visualizing text modelling results
```

In this exercise we'll be using the dataset we used for the sentiment analysis exercise. The data were collected from the Twitter accounts of the top eight newspapers in the UK by circulation. The tweets include any tweets by the news outlet from their main account. 

## Importing data

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
```

We first take a sample from these data to speed up the runtime of some of the analyses. 

```{r}
tweets <- tweets %>%
  sample_n(20000)  #该 sample_n() 函数从数据框 tweets 中随机抽样，选择20,000行。
```

## Construct `dfm` object

Then, as in the previous exercise, we create a corpus object, specify the document-level variables by which we want to group, and generate our document feature matrix. 

```{r}
#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "text")

#add in username document-level information
docvars(tweets_corpus, "newspaper") <- tweets$user_username  #推文的用户名被添加为变量名称“newspaper”下的文档级信息。

dfm_tweets <- dfm(tokens(tweets_corpus,
                    remove_punct = TRUE)) %>%
  dfm_select(pattern = stopwords("english"), 
             selection = "remove",
             valuetype = "fixed") #提取 tweets_corpus 的标记创建一个文档特征矩阵 （ dfm_tweets ）
```

We can then have a look at the number of documents (tweets) we have per newspaper Twitter account. 

```{r}

## number of tweets per newspaper
table(docvars(dfm_tweets, "newspaper"))

```

And this is what our document feature matrix looks like, where each word has a count for each of our eight newspapers. 

```{r}

dfm_tweets

```

## Estimate wordfish model

Once we have our data in this format, we are able to group and trim the document feature matrix before estimating the wordfish model.

```{r}
# compress the document-feature matrix at the newspaper level
dfm_newstweets <- dfm_group(dfm_tweets, groups = newspaper)
# remove words not used by two or more newspapers
dfm_newstweets <- dfm_trim(dfm_newstweets, 
                                min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(dfm_newstweets)

#### estimate the Wordfish model ####
set.seed(123L)  #为可重复性奠定了基础，确保随机过程的结果在不同的代码运行中保持不变。
dfm_newstweets_results <- textmodel_wordfish(dfm_newstweets, 
                                             sparse = TRUE) #sparse=TRUE指定应使用稀疏矩阵估计模型以提高计算效率。

```

And this is what results.

```{r}
summary(dfm_newstweets_results)
```

We can then plot our estimates of the $\theta$s---i.e., the estimates of the latent newspaper position---as so.

```{r}
textplot_scale1d(dfm_newstweets_results)
```

Interestingly, we seem not to have captured ideology but some other tonal dimension. We see that the tabloid newspapers are scored similarly, and grouped toward the right hand side of this latent dimension; whereas the broadsheet newspapers have an estimated theta further to the left.

Plotting the "features," i.e., the word-level betas shows how words are positioned along this dimension, and which words help discriminate between news outlets.

```{r}

textplot_scale1d(dfm_newstweets_results, margin = "features")

```

And we can also look at these features.
此代码片段实质上会生成一个表格，其中显示了具有最高 beta 值的前 20 个特征（单词或术语），这些特征表示它们在 Wordfish 模型中的重要性或关联。
```{r}

features <- dfm_newstweets_results[["features"]]  #从Wordfish模型结果中提取特征（即单词或术语），并将它们分配给变量 features

betas <- dfm_newstweets_results[["beta"]]  #与每个特征相关的 beta 值（系数）是从 Wordfish 模型结果中提取并存储在变量 betas 中

feat_betas <- as.data.frame(cbind(features, betas))  #将features和 beta 组合到一个名为 feat_betas using 的数据 cbind() 框中
feat_betas$betas <- as.numeric(feat_betas$betas)  #转换为数值类型

feat_betas %>%
  arrange(desc(betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")  #根据 betas 列按降序排列数据框 feat_betas ，选择前 20 行，然后使用 kableExtra 包中的 kbl() 函数将其格式化为表。最后，它使用 kable_styling() 将引导样式应用于表。

```

These words do seem to belong to more tabloid-style reportage, and include emojis relating to film, sports reporting on "cristiano" as well as more colloquial terms like "saucy."

## Replicating Kaneko et al.

This section adapts code from the replication data provided for @kaneko_estimating_2021 [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EL3KYD). 


If you're working locally, you can download the `dfm` data with:

```{r}
kaneko_dfm  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/study1_kaneko.rds?raw=true")))
```

This data is in the form a document-feature-matrix. We can first manipulate it in the same way as @kaneko_estimating_2021 by grouping at the level of newspaper and removing infrequent words.

```{r}
table(docvars(kaneko_dfm, "Newspaper"))
## prepare the newspaper-level document-feature matrix
# compress the document-feature matrix at the newspaper level
kaneko_dfm_study1 <- dfm_group(kaneko_dfm, groups = Newspaper)
# remove words not used by two or more newspapers
kaneko_dfm_study1 <- dfm_trim(kaneko_dfm_study1, min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(kaneko_dfm_study1)
```

## Exercises

1. Estimate a wordfish model for the @kaneko_estimating_2021 data
```{r}
# estimate the Wordfish model
set.seed(123L)
kaneko_dfm_study1_results <- textmodel_wordfish(kaneko_dfm_study1, 
                                             sparse = TRUE)
#this is what results
summary(kaneko_dfm_study1_results)

#plot estimates of the θ
textplot_scale1d(kaneko_dfm_study1_results)
```
2. Visualize the results
```{r}
#Plotting the “features”
textplot_scale1d(kaneko_dfm_study1_results, margin = "features")
```

```{r}
features <- kaneko_dfm_study1_results[["features"]]

betas <- kaneko_dfm_study1_results[["beta"]]

feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)

feat_betas %>%
  arrange(desc(betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")
```